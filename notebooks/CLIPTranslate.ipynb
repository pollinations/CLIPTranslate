{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CLIPTranslate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIdxMMMtuZQw",
        "outputId": "dd33ab3a-2554-42ef-dcdb-79530be041b0"
      },
      "source": [
        "#@title Setup\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import subprocess\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex pytorch-ignite visdom\n",
        "!pip install git+https://github.com/nielsrolf/siren-pytorch\n",
        "!pip install git+https://github.com/pollinations/AudioCLIP\n",
        "!pip install git+https://github.com/pollinations/CLIPTranslate\n",
        "\n",
        "!wget https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/bpe_simple_vocab_16e6.txt.gz -O 'AudioCLIP/assets/bpe_simple_vocab_16e6.txt.gz'\n",
        "!wget https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/AudioCLIP-Partial-Training.pt\n",
        "\n",
        "!wget https://raw.githubusercontent.com/pollinations/CLIPTranslate/main/notebooks/data/cat.jpg\n",
        "!wget https://raw.githubusercontent.com/pollinations/CLIPTranslate/main/notebooks/data/gt_bach.wav\n",
        "!wget https://raw.githubusercontent.com/pollinations/CLIPTranslate/main/notebooks/data/hearbeat.jpg\n",
        "\n",
        "\n",
        "from clip_translate.utils import load_img, imshow, load_audio, play\n",
        "sample_img = load_img(\"/content/cat.jpg\")\n",
        "imshow(sample_img)\n",
        "\n",
        "sample_audio = load_audio(\"/content/gt_bach.wav\")\n",
        "play(sample_audio)\n",
        "\n",
        "\n",
        "from siren_pytorch import SirenNet, SirenWrapperNDim\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from clip_translate import AudioImagine\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda:0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 11.0\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████████████████████         | 834.1MB 1.3MB/s eta 0:04:10tcmalloc: large alloc 1147494400 bytes == 0x555b1d924000 @  0x7f7440cdc615 0x555ae4c3fcdc 0x555ae4d1f52a 0x555ae4c42afd 0x555ae4d33fed 0x555ae4cb6988 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb67f0 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb332a 0x555ae4d34e36 0x555ae4cb2853 0x555ae4d34e36 0x555ae4cb2853 0x555ae4d34e36 0x555ae4cb2853 0x555ae4d34e36 0x555ae4db73e1 0x555ae4d176a9 0x555ae4c82cc4 0x555ae4c43559 0x555ae4cb74f8 0x555ae4c4430a 0x555ae4cb23b5 0x555ae4cb17ad 0x555ae4c443ea 0x555ae4cb23b5 0x555ae4c4430a 0x555ae4cb23b5\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7MB 1.2MB/s eta 0:01:27tcmalloc: large alloc 1434370048 bytes == 0x555b61f7a000 @  0x7f7440cdc615 0x555ae4c3fcdc 0x555ae4d1f52a 0x555ae4c42afd 0x555ae4d33fed 0x555ae4cb6988 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb67f0 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb332a 0x555ae4d34e36 0x555ae4cb2853 0x555ae4d34e36 0x555ae4cb2853 0x555ae4d34e36 0x555ae4cb2853 0x555ae4d34e36 0x555ae4db73e1 0x555ae4d176a9 0x555ae4c82cc4 0x555ae4c43559 0x555ae4cb74f8 0x555ae4c4430a 0x555ae4cb23b5 0x555ae4cb17ad 0x555ae4c443ea 0x555ae4cb23b5 0x555ae4c4430a 0x555ae4cb23b5\n",
            "\u001b[K     |████████████████████████████████| 1156.7MB 1.2MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x555bb7766000 @  0x7f7440cdc615 0x555ae4c3fcdc 0x555ae4d1f52a 0x555ae4c42afd 0x555ae4d33fed 0x555ae4cb6988 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb260e 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb260e 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb260e 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb260e 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb260e 0x555ae4c4430a 0x555ae4cb260e 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb332a 0x555ae4cb14ae 0x555ae4c443ea 0x555ae4cb332a 0x555ae4cb14ae 0x555ae4c44a81\n",
            "\u001b[K     |████████████████████████████████| 1156.8MB 16kB/s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awjIWLy-3lj-"
      },
      "source": [
        "# Fit an audio to a text or image prompt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjY_MWFu6nRw"
      },
      "source": [
        "from clip_translate import AudioImagine, get_siren_decoder, fit_siren\n",
        "\n",
        "imagine = AudioImagine(\n",
        "    perceptor=\"AudioCLIP-Partial-Training.pt\", \n",
        "    text=\"A cat\", \n",
        "    image=sample_img)\n",
        "\n",
        "siren = get_siren_decoder(sample_audio.shape, latent_dim=None)\n",
        "fit_siren(imagine, siren,  steps=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqlXZOJObFcH"
      },
      "source": [
        "# Fitting a hypernetwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDesopz7bFFB"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from clip_translate import AudioImagine, get_siren_decoder, fit_siren\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "  def __init__(self, encoder, decoder, loss):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.loss = loss\n",
        "  \n",
        "  def forward(self, audio, target=None):\n",
        "    latent = self.encoder(audio)\n",
        "    reconstructed = self.decoder(latent=latent)\n",
        "    if target is not None:\n",
        "      return self.loss(target, reconstructed)\n",
        "    return reconstructed\n",
        "\n",
        "\n",
        "latent = nn.Parameter(torch.zeros(1024).normal_(0, 1)).to('cuda')\n",
        "def constant_encoder(audio):\n",
        "  return latent\n",
        "\n",
        "\n",
        "def clip_encoder(audio):\n",
        "  audio = audio.reshape(1, -1)\n",
        "  with torch.no_grad():\n",
        "    latent = imagine.encode_audio(audio.detach())\n",
        "  return latent.reshape(1024)\n",
        "\n",
        "\n",
        "def get_siren_decoder(output_shape, latent_dim=1024):\n",
        "    net = SirenNet(\n",
        "        dim_in=1,\n",
        "        dim_hidden=256,\n",
        "        dim_out=1,\n",
        "        num_layers=3,\n",
        "        w0=30.,\n",
        "        w0_initial=10000.,\n",
        "        use_bias=True,\n",
        "        final_activation=None)\n",
        "\n",
        "    decoder = SirenWrapperNDim(\n",
        "        net,\n",
        "        latent_dim=latent_dim,\n",
        "        output_shape=output_shape\n",
        "    )\n",
        "    decoder.cuda()\n",
        "\n",
        "    return decoder\n",
        "\n",
        "\n",
        "\n",
        "def train_on_single_sample(ae, lr=1e-4, steps=2000):\n",
        "  optim = torch.optim.Adam(lr=lr, params=ae.parameters())\n",
        "  steps_till_summary = 1000\n",
        "  for step in range(steps):\n",
        "    loss = ae(sample_audio, sample_audio)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if step % steps_till_summary == 0:\n",
        "      print(loss.cpu().detach())\n",
        "      pred_audio = ae(sample_audio)\n",
        "      play(pred_audio)\n",
        "      plt.plot(pred_audio.cpu().detach().numpy().squeeze())\n",
        "      plt.show()\n",
        "\n",
        "# decoder = get_siren_decoder(sample_audio.shape, 1024)\n",
        "# ae = Autoencoder(encoder=clip_encoder, decoder=decoder, loss=F.mse_loss)\n",
        "# train_on_single_sample(ae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BpZnS6oLJUs"
      },
      "source": [
        "class GON(Autoencoder):\n",
        "    def __init__(self, decoder):\n",
        "        super().__init__(encoder=self.encode, decoder=decoder, loss=F.mse_loss)\n",
        "    \n",
        "    def encode(self, audio):\n",
        "        latent = nn.Parameter(torch.zeros(1024)).to('cuda')\n",
        "        inner_loss = self.loss(audio, self.decoder(latent=latent))\n",
        "        z = -torch.autograd.grad(inner_loss, [latent], create_graph=True, retain_graph=True)[0]\n",
        "        return z\n",
        "\n",
        "decoder = get_siren_decoder(sample_audio.shape, 1024)\n",
        "ae = GON(decoder=decoder)\n",
        "train_on_single_sample(ae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlmhkmx8i9j1"
      },
      "source": [
        "# Train GON on multiple audio examples\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "audio_files = glob(\"/content/drive/MyDrive/ddsp/samples/*/*.wav\")\n",
        "audio_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpplvg2blcyO"
      },
      "source": [
        "audios = [load_audio(i) for i in audio_files]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl5YKseLnbqe"
      },
      "source": [
        "audios[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_w1sC0InCIA"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def random_crop(audio, seconds=4):\n",
        "    audio_shape = audio.shape\n",
        "    frames = int(seconds * rate)\n",
        "    cutoff = audio.shape[0] - frames\n",
        "    cutoff_start = np.random.randint(0, cutoff)\n",
        "    cutoff_end = cutoff - cutoff_start\n",
        "    audio = audio[cutoff_start:-cutoff_end]\n",
        "    return audio\n",
        "\n",
        "def get_sample():\n",
        "    audio = audios[np.random.randint(len(audios))]\n",
        "    return random_crop(audio, seconds=4)\n",
        "\n",
        "\n",
        "for _ in range(4):\n",
        "    play(get_sample())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BzlrLRWln2Kn"
      },
      "source": [
        "steps = 10000\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder = get_siren_decoder(get_sample().shape, 1024)\n",
        "ae = GON(decoder=decoder)\n",
        "\n",
        "optim = torch.optim.Adam(lr=lr, params=ae.parameters())\n",
        "steps_till_summary = 1000\n",
        "for step in range(steps):\n",
        "    x = get_sample()\n",
        "    loss = ae(x, x)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if step % steps_till_summary == 0:\n",
        "        print(loss.cpu().detach())\n",
        "        pred_audio = ae(x)\n",
        "        play(pred_audio)\n",
        "        plt.plot(pred_audio.cpu().detach().numpy().squeeze())\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txsXqvIbog0O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}